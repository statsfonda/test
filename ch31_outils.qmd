
# Outils pour construire des intervalles de confiance


## Quantiles {#sec-quantiles}

Si $X$ est une variable aléatoire sur $\mathbb{R}$, un quantile d'ordre $\beta \in ]0,1[$, noté $q_\beta$, est un nombre tel que $\mathbb{P}(X \leqslant q_\beta) = \beta$. Lorsque $X$ est continue, un tel nombre existe forcément, car la fonction de répartition $F(x) = \mathbb{P}(X\leqslant x)$ est une surjection continue. Les quantiles symétriques $z_\beta$ sont, eux, définis par $\mathbb{P}(|X|\leqslant z_\beta) = \beta$. 

Si la loi de $X$ est de surcroît symétrique, les quantiles symétriques s'expriment facilement en fonction des quantiles classiques. En effet, $\mathbb{P}(|X|\leqslant z)$ est égal à $\mathbb{P}(X \leqslant z) - \mathbb{P}(X \leqslant -z)$. Or, si la loi de $X$ est symétrique, alors $\mathbb{P}(X \leqslant -z) = 1 - \mathbb{P}(X \leqslant z)$, et donc 
$$ \mathbb{P}(|X|\leqslant z) = 2\mathbb{P}(X \leqslant z) - 1.$$
Il suffit alors de choisir pour $z$ le quantile $q_{\frac{1+\beta}{2}}$ pour obtenir $\mathbb{P}(|X|\leqslant z) = \beta$. Lorsque $\beta$ est de la forme $1-\alpha$ avec $\alpha$ petit (comme les niveaux des intervalles de confiance), on trouve alors $z_{1-\alpha} = q_{1 - \alpha/2}$. 

Les quantiles s'obtiennent en inversant la fonction de répartition : lorsque celle-ci est une bijection sur $]0,1[$, alors $q_\beta = F^{-1}(\beta)$. En règle générale, il n'y a pas de forme fermée. Par exemple, pour une loi gaussienne standard, 
$$F(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^x e^{-u^2/2}du$$
qui elle-même n'a pas d'écriture plus simple. Fort heureusement, les outils de calcul numérique permettent d'effectuer ces calculs avec une grande précision. La table suivante donne les quantiles symétriques de la gaussienne.  

| $\beta$ | 90% | 95% | 98% | 99% | 99.9% | 99.99999% |
| ---- | - | - | - | - | - | - |
| $z_\beta$ | 1.64 | 1.96 | 2.32 | 2.57 | 3.2 | 5.32 |

Voir aussi la [règle 1-2-3](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule). Il existe de nombreuses tables de quantiles pour les lois usuelles. 



:::{#thm-gaussiantail}

## Queues de distribution de la gaussienne

Si $x$ est plus grand que 1, 
$$  \left(\frac{1}{x} - \frac{1}{x^3}\right) \frac{e^{-x^2/2}}{\sqrt{2\pi}}\leqslant \mathbb{P}(X > x) \leqslant \frac{1}{x}\frac{e^{-x^2/2}}{\sqrt{2\pi}} $$
En particulier, si $x$ est grand, $\mathbb{P}(X \geqslant x) \sim e^{-x^2/2}/x\sqrt{2\pi}$ avec une erreur d'ordre $O(e^{-x^2/2}/x^3)$. 
:::

À titre d'exemple, pour $x=2.32$ cette approximation donne 98.83%, ce qui est remarquablement proche de 98%. Pour $x = 2.57$ on trouve 99.42%.

:::{.proof}

À écrire. 
:::


## Calculs de lois

### Lois Gamma 

Une variable aléatoire suit une loi Gamma de paramètres $\lambda>0, \alpha>0$ lorsque sa densité est donnée par 
$$\gamma_{r,\alpha}(x) =  \frac{\lambda^\alpha}{\Gamma(\alpha)}e^{-\lambda x}x^{\alpha -1}\mathbf{1}_{x>0}.$$
Les lois Gamma rassemblent les lois exponentielles ($\Gamma(\lambda, 1) = \mathscr{E}(\lambda)$) et les lois du chi-deux qu'on verra ci-dessous $(\Gamma(1/2, n/2) = \chi_2(n)$). La transformée de Fourier $\varphi_{\lambda, \alpha}$ d'une loi $\Gamma(\lambda, \alpha)$ se calcule facilement par un changement de variables : 
$$\varphi_{\lambda, \alpha}(t) = \left(1 - \frac{it}{\lambda}\right)^{-\alpha}.$${#eq-fcgamma}
Cette identité montre également que si $X_1, \dotsc, X_n$ sont des variables indépendantes de loi $\Gamma(\lambda, \alpha_i)$, alors leur somme est une variable de loi $\Gamma(\lambda, \alpha_1 + \dotsc + \alpha_n)$. 

### Loi du chi-deux {#sec-chideux}

Soit $X$ une loi gaussienne standard. Calculons la densité de $X^2$ ; pour toute fonction-test $\varphi$, $\mathbb{E}[\varphi(X^2)]$ est donné par
$$\frac{1}{\sqrt{2\pi}}\int e^{-x^2/2}\varphi(x^2)dx.$$
Cette intégrale est symétrique, donc on peut ajouter un facteur 2 et intégrer sur $[0,\infty[$. En posant $u=x^2$, on obtient alors la valeur 
$$ \frac{2}{\sqrt{2\pi}}\int_0^\infty e^{-u/2}\varphi(u)\frac{1}{2\sqrt{u}}du.$$
On reconnaît la densité d'une [loi Gamma](https://fr.wikipedia.org/wiki/Loi_Gamma) de paramètres $(1/2, 1/2)$. Cette loi s'appelle *loi du chi-deux* et on la note $\chi_2(1)$. Sa tranformée de Fourier est donnée par 
$$\mathbb{E}[e^{itX^2}] = \frac{1}{\sqrt{1 - 2it}}. $$

Soient maintenant $X_1,\dotsc, X_n$ des lois gaussiennes standard indépendantes. Chaque $X_i^2$ est une $\chi_2(1)$ ; leur somme a donc pour loi la convolée $n$ fois de $\chi_2$. Calculons sa tranformée de Fourier : 
\begin{align}\mathbb{E}[e^{it(X_1^2 + \dotsb + X_n^2)}] &= \mathbb{E}[e^{itX_1^2}]^n \\ &= (1-2it)^{-\frac{n}{2}} .\end{align}
On reconnaît la transformée de Fourier d'une loi $\Gamma(n/2, 1/2)$ ; cette loi s'appelle loi du chi-deux à $n$ paramètres de liberté et elle est notée $\chi_2(n)$. Sa densité est donnée par
$$ \frac{1}{2^{n/2}\Gamma(n/2)}e^{-x/2}x^{n/2 - 1}\mathbf{1}_{x>0}.$$

### Loi de Student {#sec-student1}

Soit $X$ une variable aléatoire gaussienne standard et $Y_n$ une variable aléatoire suivant une loi $\chi_2(n)$ indépendante de $X$. 
On va calculer la loi de $T_n = X/\sqrt{Y_n/n}$. Soit $f$ une fonction test ; l'espérance $\mathbb{E}[f(T_n)]$ est égale à 
$$\frac{1}{Z_n\sqrt{2\pi}}\int_0^\infty \int_{-\infty}^{\infty}  f\left(\frac{x}{\sqrt{y/n}}\right) e^{-\frac{x^2}{2}}e^{-\frac{y}{2}}y^{\frac{n}{2} - 1}dxdy $$
où $Z_n = 2^{n/2}\Gamma(n/2)$. Dans l'intégrale en $x$, on effectue le changement de variable $u = x/\sqrt{y/n}$ afin d'obtenir 
$$\frac{1}{Z_n\sqrt{2\pi}}\int_0^\infty \int_{-\infty}^{\infty}  f(u) e^{-\frac{yu^2}{2n}}e^{-\frac{y}{2}}y^{\frac{n}{2} - 1}\sqrt{\frac{y}{n}} dxdy. $$
La densité de $T_n$ est donc donnée par 
$$t_n(u)= \frac{1}{Z_n\sqrt{2\pi n}}\int_0^\infty  e^{-\frac{yu^2}{2n}-\frac{y}{2}}y^{\frac{n+1}{2}-1} dy. $$
Le changement de variables $z = y(1+u^2/n)/2$ nous ramène à une fonction Gamma : 
$$t_n(u) = \frac{1}{Z_n\sqrt{2\pi n}}\left(\frac{2}{1+\frac{u^2}{n}}\right)^{\frac{n+1}{2}}\int_0^\infty  e^{-z}z^{\frac{n+1}{2}- 1} dz.$$
On reconnaît $\Gamma((n+1)/2)$ à droite. La densité $t_n(x)$ est donc égale à 
$$t_n(x) = \frac{1}{\sqrt{n\pi}}\frac{\Gamma\left(\frac{n+1}{2}\right)}{\Gamma\left(\frac{n}{2}\right)}\left(\frac{1}{1 + \frac{u^2}{n}}\right)^{\frac{n+1}{2}}.$$

Cette loi s'appelle *loi de Student* de paramètre $n$ ; on dit parfois *à $n$ degrés de liberté*. Elle est notée $\mathscr{T}(n)$. La loi de Student de paramètre $n=1$ est tout simplement une loi de Cauchy. 

### Loi de la statistique de Student {#sec-student2}

Soient $X_1, \dotsc, X_n$ des variables gaussiennes $N(\mu, \sigma^2)$ indépendantes, et soit $T_n = (\bar{X}_n-\mu)/\sqrt{\hat{\sigma}^2_n}$, où 
$$\hat{\sigma}^2_n = \frac{\sum_{i=1}^n (X_i - \bar{X}_n)^2}{n-1}. $$


:::{#thm-student_density}

$$T_n \sim \mathscr{T}(n-1).$$

:::

$~~$

:::{.proof}

On va montrer  1° que $\bar{X}_n$ et $\sqrt{\hat{\sigma}^2_n / \sigma^2}$ sont indépendantes, et 2° que $\sqrt{\hat{\sigma}^2_n / \sigma^2}$ a bien la même loi que $\sqrt{Y_{n-1}/(n-1)}$ où $Y_{n-1}$ est une $\chi_2(n-1)$. Dans la suite, on supposera que $\mu=0$ et $\sigma=1$, ce qui n'enlève rien en généralité. 

*Premier point. * Le vecteur $X=(X_1, \dotsc, X_n)$ est gaussien. Posons $Z = (X_1 - \bar{X}_n, \dotsc, X_n - \bar{X}_n)$.  Le couple $(\bar{X}_n, Z_n)$ est linéaire en $X$, donc ce couple est aussi un vecteur gaussien. Or, la covariance de ses deux éléments est nulle. Par exemple, $\mathrm{Cov}(\bar{X}_n, Z_1)$ est égale à $\mathrm{Cov}(\bar{X}_n, X_1) - \mathrm{Var}(\bar{X}_n)$, ce qui par linéarité donne $1/n - 1/n = 0$. Ainsi, $\bar{X}_n$ et $Z$ sont deux variables conjointement gaussiennes et décorrélées : elles sont donc indépendantes. Comme $\hat{\sigma}_n$ est une fonction de $Z$, elle est aussi indépendante de $\bar{X}_n$. 

*Second point. * $Z$ est la projection orthogonale de $X$ sur le sous-espace vectoriel $\mathscr{V}=\{x \in \mathbb{R}^n : x_1 + \dotsc + x_n = 0\}$. Soit $(f_i)_{i=2, \dotsc, n}$ une base orthonormale de $\mathscr{V}$, de sorte que $Z = \sum_{i=2}^n \langle f_i, X\rangle f_i$. Par l'identité de Parseval, 
$$|Z|^2 = \sum_{i=2}^n |\langle f_i, X \rangle|^2.$$
Or, les $n-1$ variables aléatoires $G_i = \langle f_i, X\rangle$ sont des gaussiennes standard iid. En effet, on vérifie facilement que $\mathrm{Cov}(G_i, G_j) = \langle f_i, f_j\rangle = \delta_{i,j}$. On en déduit donc que $|Z|^2$ suit une loi $\chi_2(n-1)$. 

:::

La seconde partie de la démonstration est un cas particulier du théorème de Cochran, que nous verrons dans le chapitre sur la régression linéaire. 



## Inégalités de concentration

Les outils de base pour construire des intervalles de confiance dans des circonstances générales (non gaussiennes) sont les inégalités de concentration. Une inégalité de concentration pour une variable aléatoire intégrable $X$ consiste à borner $\mathbb{P}(|X - \mathbb{E}[X]|>x)$ par quelque chose de petit quand x est grand : on cherche à contrôler la probabilité pour que les réalisations de la variable aléatoire $X$ soient éloignées de leur valeur moyenne $\mathbb{E}[X]$ de plus de $x$. 


## Inégalité de Bienaymé-Tchebychev


:::{#thm-bt}

Soit $X$ une variable aléatoire de carré intégrable. Alors, 
$$ \mathbb{P}(|X - \mathbb{E}[X]|\geqslant x)\leqslant \frac{\mathrm{Var}(X)}{x^2}.$$ 

:::



:::{.proof} 
Élever au carré les deux membres de l'inégalité dans $\mathbb{P}$, puis appliquer l'inégalité de Markov à la variable aléatoire positive $|X - \mathbb{E}X|^2$ dont l'espérance est $\mathrm{Var}(X)$. 
:::

## Inégalité de Hoeffding


:::{#thm-hoeffding}

## Inégalité de Hoeffding

Soient $X_1, \dotsc, X_n$ des variables aléatoires indépendantes, pas forcément de même loi. On suppose que chaque $X_i$ est à valeurs dans un intervalle borné $[a_i, b_i]$ et on pose $S_n = X_1 + \dotsc + X_n$. Pour tout $t>0$, 

$$\mathbb{P}(S_n - \mathbb{E}[S_n] \geqslant x) \leqslant e^{-\frac{2x^2}{\sum_{i=1}^n(b_i - a_i)^2}}$${#eq-hoeffding}
et
$$\mathbb{P}(|S_n - \mathbb{E}[S_n]| \geqslant x) \leqslant 2e^{-\frac{2x^2}{\sum_{i=1}^n(b_i - a_i)^2}}. $${#eq-hoeffdingsym}


:::

La démonstration se fonde sur le lemme suivant. 

:::{#lem-hoeffding}

## lemme de Hoeffding

Soit $X$ une variable aléatoire centrée à valeurs dans $[a,b]$. Pour tout $t$, 

$$\mathbb{E}[e^{tX}] \leqslant e^{\frac{t^2(b-a)^2}{8}}.$$

:::

:::{.proof}
Soit $X$ une variable aléatoire centrée à valeurs dans l'intervalle $[-1,1]$. En écrivant $x = (-1)\times (1-x)/2 + 1\times (1+x)/2$ et en utilisant la convexité de la fonction $x \mapsto e^{tx}$, on obtient $e^{tX}\leqslant ((1-X)e^{-t} + (1+X) e^{t})/2$, puis en prenant l'espérance et le fait que $X$ est centrée, $\mathbb{E}[e^{tX}]\leqslant \cosh(t)$. Il est facile de voir que $\cosh(t) \leqslant e^{t^2/2}$ en utilisant le développement en série entière de $\cosh$.  Le cas général où $X$ est à valeurs dans $[a,b]$ se fait de la même façon. 
:::


*Preuve de l'inégalité de Hoeffding.*
En remplaçant $X_k$ par $X_k - \mathbb{E}[X_k]$, on peut supposer que tous les $X_i$ sont centrés et étudier seulement $\mathbb{P}(S_n >t)$. 
Écrivons $\mathbb{P}(S_n > t) = \mathbb{P}(e^{\lambda S_n} > e^{\lambda t})$, où $\lambda$ est un nombre positif que l'on choisira plus tard. L'inégalité de Markov borne cette probabilité par $\mathbb{E}[e^{\lambda S_n}]e^{-\lambda t}$. Comme les $X_i$ sont indépendantes, $\mathbb{E}[e^{tS_n}]$ est le produit des $e^{ \varphi_k(\lambda)}$ où $\varphi_k(t) = \ln \mathbb{E}[e^{itX_k}]$. En appliquant le lemme de Hoeffding à chaque $\varphi_k$, on borne $\mathbb{P}(S_n >t)$ par 
$$ \exp\left(\sum_{i=1}^n \frac{(b_i - a_i)^2 \lambda^2}{8} - t\lambda\right).$$
Le minimum en $\lambda$ du terme dans l'exponentielle est atteint au point $4t / \sum (a_i - b_i)^2$ et la valeur du minimum est le terme dans l'exponentielle de @eq-hoeffding. On déduit @eq-hoeffdingsym par une simple borne de l'union.  