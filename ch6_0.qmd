# Modèles exponentiels

## Exemples {.unnumbered}
Jusqu'ici, nous avons vu de nombreux exemples de modèles statistiques. Dans la plupart des cas, il s'agissait de modèles de la loi de $n$ variables aléatoires indépendantes et identiquement distribuées selon une même loi $P$ (le modèle était donc $P^{\otimes n}$). Cette loi $P$ possède souvent une densité par rapport à une mesure de référence. Par exemple, la loi gaussienne a une densité par rapport à la mesure de Lebesgue sur $\mathbb{R}$ : 
$$ \frac{1}{\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2}}=\frac{1}{\sqrt{2\pi}e^{\frac{\mu^2}{2}}}e^{-\frac{x^2}{2}}e^{x\mu}.$$
La loi exponentielle a une densité par rapport à la mesure de Lebesgue sur $\mathbb{R}_+$ : 
$$ \frac{1}{1/\lambda}e^{-\lambda x}$$
Les lois discrètes ont une densité par rapport à la mesure de comptage : la loi de Bernoulli, par exemple, s'écrit 
$$ p^n(1-p)^{1-n} = \frac{e^{n\ln(p/(1-p))}}{(1-p)^{-1}} $$
avec $n$ valant zéro ou 1, ou encore la loi de Poisson 
$$ e^{-\lambda}\frac{\lambda^n}{n!} = \frac{1}{e^{e^{\ln(\lambda)}}}\frac{1}{n!}e^{\ln(\lambda)n}$$
ou enfin la loi géométrique
$$p^n(1-p) = \frac{e^{n\ln(p)}}{(1-p)^{-1}}.$$
Dans tous ces exemples, j'ai volontairement écrit la densité de façon inhabituelle : tous ces modèles peuvent s'écrire sous la forme 
$$ \frac{1}{Z(\theta)}h(x)e^{\theta f(x)}$$
où $f$ et $g$ sont des fonctions qui ne dépendent pas de $\theta$, et où $Z$ est une constante qui ne dépend que de $\theta$. Ces modèles appartiennent à la famille des *modèles exponentiels*. 

## Définitions

Soit $\nu$ une mesure de référence ($\sigma$-finie) sur $\mathbb{R}^d$. 

Soit $\Theta \subset \mathbb{R}^p$ (l'espace des paramètres) et soit $T : \mathbb{R}^d \to \mathbb{R}^p$ une fonction mesurable. 

On suppose que pour tout $\theta \in \Theta$, la fonction $x \mapsto e^{\langle \theta, T(x)\rangle}$ est intégrable par rapport à $\nu$ : son intégrale
$$Z_\theta = \int e^{\langle \theta, T(x)\rangle}\nu(dx)$$
est appelée *fonction de partition*. 



:::{#def-modele_expo}

Le modèle exponentiel associé à $T$ est la famille de densités (par rapport à $\nu$) définie par 
$$ p_\theta(x) = \frac{e^{\langle \theta, T(x)\rangle}}{Z_\theta}.$$

:::

Lorsqu'on fixe un $x$ dans l'espace des observations, la fonction  
$$ \theta \mapsto p_\theta(x)$$
est appelée *vraisemblance* et son logarithme
$$\ell_\theta(x) = \ln p_\theta(x)$$
 est appelé *log-vraisemblance*. Lorsqu'il est bien défini, le gradient (en $\theta$) de la log-vraisemblance 
 $$ \nabla \ln p_\theta(x)$$
 est appelé *fonction de score* du modèle. Ces termes ne sont pas propres aux modèles exponentiels. On omet fréquemment de noter la dépendance en les observations, qu'on suppose fixées une bonne fois pour toutes. 

## Retour sur des exemples


:::{#exm-s}

## Gaussiennes

La densité de $N(\mu,\sigma^2)$ s'écrit 

$$\frac{e^{-\frac{\mu^2}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}}e^{ \frac{\mu}{\sigma^2} x - \frac{1}{2 \sigma^2} x^2 }.$$
La mesure $\nu$ est la mesure de Lebesgue sur $\mathbb{R}$. Le moment $T$ est
$$T(x) = \begin{bmatrix}x \\ -x^2/2 \end{bmatrix} .$$
Le bon paramètre $\theta$ est 
$$ \theta = \begin{bmatrix}\mu/\sigma^2 \\ 1 / \sigma^2\end{bmatrix}.$$
La fonction de partition $\exp\left(\frac{\mu^2}{2 \sigma^2} \right)\sqrt{2 \pi \sigma^2}$ s'écrit donc aussi $$Z(\theta)=\exp(\theta_1^2 / 2\theta_2)\sqrt{2\pi / \theta_2}. $$

:::

L'exemple de la loi Gaussienne montre qu'en règle générale, il peut être nécessaire de « reparamétriser » le modèle pour l'écrire sous sa forme exponentielle. 

:::{#exm-ss}

## Poisson

La mesure $\nu$ est la mesure de comptage sur $\mathbb{N}$. Le paramètre est 
$$ \theta = \begin{bmatrix}\ln(\lambda) \\ 1 \end{bmatrix}$$
et le moment $T$ est 
$$ T = \begin{bmatrix}-n \\ - \ln(n!)\end{bmatrix}$$
de sorte que la fonction de partition est $Z(\theta) = e^{e^{\ln\lambda}} = e^{e^{\theta_1}}$.


:::

## Régularité

On supposera dorénavant que l'espace des paramètres $\Theta$ (qui est une partie de $\mathbb{R}^p$) est un ouvert, et que $Z(\theta)$ est fini pour tout $\theta \in \Theta$. Cela sera aussi valable pour les sections suivantes. 

:::{#prp-reg}

1) Pour tout $n$, $E_\theta[|T(X)|^n]$ est fini. 

2) La fonction de partition d'un modèle exponentiel est infiniment différentiable. 

3) Le gradient de la log-partition est donné par 
$$\nabla \ln Z(\theta) = E_\theta[T(X)]$${#eq-grad-exp}
et sa matrice hessienne^[On rappelle que la Hessienne d'une fonction $f:\mathbb{R}^d \to \mathbb{R}$ est la matrice de ses dérivées secondes $$\frac{\partial^2}{\partial x_i \partial x_j}f(x_1, \dotsc, x_d).$$
Par abus de notation, on la note souvent $\nabla^2$, mais il serait plus juste de la noter $\nabla^\top \nabla$. ] par 
$$ \nabla^2 \ln Z(\theta) = \mathrm{Var}_\theta(T(X)).$${#eq-hess-exp}

:::

:::{.proof}

1. Prenons un petit $\delta$ tel que $\theta + \delta$ et $\theta - \delta$ sont dans $\Theta$. Comme $Z(\theta \pm \delta) = \int e^{\langle \theta, T(x)\rangle \pm \langle \delta, T(X)\rangle}\nu(dx)$ et que $e^{|\langle \delta, T(x)\rangle|}\leqslant e^{\langle \delta, T(x)\rangle} + e^{\langle -\delta, T(x)\rangle}$, on en déduit que 
$$\int e^{\langle \theta, T(x)\rangle + |\langle \delta, T(x)\rangle|}\nu(dx)<\infty. $$
 Le théorème d'interversion série-intégrale à termes positif montre que ce terme est aussi égal à 
$$ \sum_{n=0}^\infty \int e^{\langle \theta, T(x)\rangle}\frac{|\langle \delta, T(x)\rangle|^n}{n!}\nu(dx).$$
Tous les termes de cette somme sont donc finis, ce qui signifie précisément que $E_\theta[|\langle \delta, T(X)\rangle|^n]<\infty$ pour tout $n$. Ceci étant valable pour tout $\delta$ dans une boule autour de $\theta$, il est immédiat d'en déduire que $E_\theta[|\langle x, T(X)\rangle|^n]$ est fini pour tout $n$ et pour tout $x$. En prenant $x = \delta_i$, on voit donc que les coordonnées de $T$ ont des moments finis à tous les ordres, et donc que $T$ possède des moments finis à tous les ordres au sens où $E_\theta[|T(X)|^n]<\infty$. 


1. On a $\nabla \ln Z(\theta) = \nabla Z(\theta)/Z(\theta)$. Formellement, $\nabla Z(\theta)$ est donc égal à 
$$\nabla \int e^{\langle \theta, T\rangle} = \int \nabla e^{\langle \theta, T\rangle} = \int T e^{\langle \theta, T\rangle}. $$
Il est alors clair que $\nabla \ln Z(\theta) = \int p_\theta T$. Pour justifier la dérivation sous le signe intégral, notons $f(\theta, x) = e^{\langle \theta, T(x)\rangle}$. Elle est infiniment différentiable en $\theta$ et son intégrale est finie dès que $\theta$ est dans $\Theta$. Son gradient en $\theta$ est égal à $e^{\langle \theta, T(x)\rangle}T(x)$ qui est d'intégrale finie d'après le premier point. En regardant bien la démonstration, on constate également qu'il y a une constante $c$ telle que pour tout $\delta$ dans un voisinage de $\theta$, on a $E_{\theta + \delta}[|T(X)|]<c$. Tout cela permet d'appliquer le théorème de dérivation sous le signe intégral et la formule @eq-grad-exp. 

1. Pour la formule @eq-hess-exp, c'est la même chose : $\nabla^2 \ln Z(\theta) = \nabla \frac{\nabla Z(\theta)}{Z(\theta)}$. Les règles de dérivation des produits disent alors que ce terme est égal à 
$$\frac{Z(\theta)\nabla^2 Z(\theta) - \nabla Z(\theta)\nabla Z(\theta)^\top}{Z(\theta)^2}. $$
Il suffit donc de calculer $\nabla^2 Z(\theta)$, qui par un argument de dérivation sous l'intégrale similaire au précédent, est égal à 
$$ \int e^{\langle \theta, T(x)\rangle}T(x)T(x)^\top \nu(dx).$$
La formule @eq-hess-exp découle alors de la définition de la covariance. 

:::

## Identifiabilité

:::{#thm-iden-expo}

Dans un modèle exponentiel, les points suivants sont équivalents. 

i) Le modèle est identifiable. 
ii) La matrice hessienne de la fonction de log-partition (@eq-hess-exp) est inversible en tout $\theta$. 
iii)  $\nabla \ln Z(\theta)$ est un difféomorphisme. 
:::

:::{.proof}

Démontrons d'abord l'équivalence des deux premiers points. 

La matrice hessienne de $\ln Z_\theta$ est égale à $\mathrm{Var}_\theta(T(X))$, donc elle est toujours positive. Si elle n'est pas inversible, alors elle n'est pas définie positive et il existe un $\mu$ tel que $\mu^\top \mathrm{Var}_\theta(T) \mu$ vaut zéro. Or, ce terme est aussi égal à $\mathrm{Var}_\theta(\langle \mu, T\rangle)$. Cela impliquerait que la variable aléatoire $\langle \mu, T(X)\rangle$ soit constante $P_\theta$-presque sûrement, disons égale à un certain $\alpha$, et donc que $\nu$-presque sûrement, $\langle \mu, T(x)\rangle = \alpha$. Mais alors, $p_{\theta + \mu}(x)$ peut s'écrire
$$\frac{e^{\langle\theta + \mu, T(x)\rangle}}{Z(\theta + \mu)} = \frac{e^{\langle\theta, T(x)\rangle}e^{\alpha}}{Z(\theta + \mu)}$$
c'est-à-dire 
$$p_\theta(x)\times \frac{e^\alpha Z(\theta)}{Z(\theta + \mu)}.$$
Or, comme $p_\theta$ est une densité de probabilité, son intégrale vaut 1 : la constante $e^\alpha Z(\theta) / Z(\theta + \mu)$ vaut donc 1 et l'on a montré que $p_{\theta + \mu}$ et $p_\theta$ sont égales partout. Le modèle n'est donc pas identifiable. 

Pour l'autre sens, il suffit de reprendre toutes ces implications à l'envers : si $p_{\theta + \mu} = p_\theta$, alors pour $\nu$-presque tout $x$ on aura $\langle \theta + \mu, T(x)\rangle = \langle \theta, T(x)\rangle$, et donc $\langle \mu, T(x)\rangle = 0$, et in fine $\mu^\top \mathrm{Var}_\theta(T(X))\mu = 0$. 


Comme iii) entraîne ii), il suffit donc de montrer que i) et ii) entraînent iii). Nous allons montrer la contraposée : si iii) n'est pas vrai et que ii) n'est pas vrai, c'est fini. On peut donc supposer que iii) n'est pas vrai et que ii) est vrai, et il faut montrer que i) est faux. Le point ii) entraîne que $\nabla \ln Z$ est localement injective (théorème d'inversion locale), donc si cette application n'était pas un difféomorphisme, cela voudrait dire qu'elle n'est pas injective et qu'il y aurait donc $\theta\neq\mu$ tels que $\nabla \ln Z(\theta) = \nabla \ln Z(\mu)$. Or, un calcul montre que la divergence de Kullback-Leibler (@def-dkl) *symétrisée*, à savoir $\dkl(P_\theta \mid P_\mu) + \dkl(P_\mu \mid P_\theta)$, est égale à  $$\langle \nabla \ln Z(\theta) - \nabla \ln Z(\mu), \theta - \mu \rangle $${#eq-dkl-expo-formule}
et ceci vaut donc zéro : c'est donc que chacune des deux $\dkl$ vaut zéro, puisque ces deux termes sont positifs. On en déduit alors que $P_\theta = P_\mu$, et donc le modèle n'est pas identifiable. 
:::

*Preuve de @eq-dkl-expo-formule.* En utilisant seulement les définitions, $\dkl(P_\theta \mid  P_\mu)$ est égal à 
$$\int p_\theta(x)(\langle \theta, T(x)\rangle - \langle \mu, T(x)\rangle) \nu(dx) - \ln Z(\theta) + \ln Z(\mu). $$
Le premier terme est égal à $\langle \theta - \mu, E_\theta[T]\rangle$. 
La somme $\dkl(P_\theta \mid P_\mu) + \dkl(P_\mu \mid P_\theta)$ se simplifie et se factorise donc en 
$$\langle \theta - \mu, E_\theta[T] - E_\mu[T]\rangle $$
et l'identité en découle puisque $\nabla \ln Z(\theta) = E_\theta[T]$. 


<!--
## Statistiques exhaustives

Une statistique $T$ est *exhaustive* lorsqu'il existe deux fonctions $g,h$ telles que
$$ p_\theta(x) = g(x)h(T(x), \theta).$$ 
Lorsqu'une statistique exhaustive existe, elle contient toute l'information sur $\theta$ que l'on peut obtenir à partir d'une observation $x$. -->